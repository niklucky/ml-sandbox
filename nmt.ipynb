{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNt1SWEbi/QD9Coz8xgQjRe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niklucky/ml-sandbox/blob/main/nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: mt5\n",
        "Attention based model"
      ],
      "metadata": {
        "id": "2xM7JHpcZLyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation models\n",
        "\n",
        "This is a playground where I'm playing around with with language models\n",
        "\n",
        "## Materials\n",
        "\n",
        "* Transformer paper: https://arxiv.org/abs/1706.03762\n",
        "* mT5 paper: https://arxiv.org/abs/2010.11934\n",
        "* Article on how transformers work: http://towardsdatascience.com/transformers-141e32e69591\n"
      ],
      "metadata": {
        "id": "5rScSmexV2fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!pip install transformers sentencepiece datasets"
      ],
      "metadata": {
        "id": "cglHxtU_ZUc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from google.colab import drive\n",
        "from IPython.display import display\n",
        "from IPython.html import widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "sns.set()"
      ],
      "metadata": {
        "id": "rNgc8bJgabS_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "model_repo = \"google/mt5-small\"\n",
        "max_seq_len = 20"
      ],
      "metadata": {
        "id": "6PoBXFgcaIbz"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_repo)"
      ],
      "metadata": {
        "id": "ZxSHxe3saSrT"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model description https://huggingface.co/google/mt5-small\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_repo)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "lvCpJWTllcWO"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Here is our test sentence!\"\n",
        "token_ids = tokenizer.encode(input_sentence, return_tensors='pt').cuda()\n",
        "\n",
        "token_ids\n",
        "\n",
        "model_out = model.generate(token_ids)\n",
        "print(model_out)\n",
        "\n",
        "output_text = tokenizer.convert_tokens_to_string(\n",
        "    tokenizer.convert_ids_to_tokens(model_out[0])\n",
        ")\n",
        "\n",
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srl-qoBXl_Os",
        "outputId": "602078d3-bd34-46e7-d5d3-28520486f081"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[     0, 250099,      1]], device='cuda:0')\n",
            "<pad> <extra_id_0></s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps\n",
        "1. Load pretrained model and tokenizer\n",
        "2. Load in the dataset\n",
        "3. Transform dataset into input\n",
        "4. Train/finetune the model on our dataset\n",
        "5. Test the model"
      ],
      "metadata": {
        "id": "72gMKzlLqRPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_str = '<jp>This is a test nguig.'\n",
        "input_ids = tokenizer.encode(example_input_str, return_tensors='pt')\n",
        "print('Input IDs:', input_ids)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neZ7wzXNrRqe",
        "outputId": "4c657883-0481-4774-f7b8-09b1e7530448"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[ 1042,  3889,   669, 13673,   339,   259,   262,  2978,   259,  1180,\n",
            "          1315,   260,     1]])\n",
            "['▁<', 'jp', '>', 'This', '▁is', '▁', 'a', '▁test', '▁', 'ngu', 'ig', '.', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sorted(tokenizer.vocab.items(), key=lambda x: x[1])\n",
        "\n",
        "dataset = load_dataset('alt')"
      ],
      "metadata": {
        "id": "UPls2Vd352o3"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "# To see model mapping\n",
        "# train_dataset[0]"
      ],
      "metadata": {
        "id": "l0ZSS6A96uLj"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LANG_TOKEN_MAPPING = {\n",
        "    'en': '<en>',\n",
        "    'ru': '<ru>',\n",
        "    'ja': '<jp>',\n",
        "}"
      ],
      "metadata": {
        "id": "tofqbPjU7Fuk"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens_dict = {'additional_special_tokens': list(LANG_TOKEN_MAPPING.values())}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0wadW8f74jL",
        "outputId": "0084b268-5ced-4324-b9c2-30814eb66755"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 250104. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(250104, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer.encode(\n",
        "    example_input_str,\n",
        "    return_tensors='pt',\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=max_seq_len\n",
        "  )\n",
        "print(token_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhnngMP59LKn",
        "outputId": "ed218c7b-1902-4813-9bf5-8058c87a09c5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1042,  3889,   669, 13673,   339,   259,   262,  2978,   259,  1180,\n",
            "          1315,   260,     1,     0,     0,     0,     0,     0,     0,     0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_input_str(\n",
        "    text, target_lang, tokenizer, seq_len,\n",
        "    lang_token_map=LANG_TOKEN_MAPPING):\n",
        "  target_lang_token = lang_token_map[target_lang]\n",
        "\n",
        "  # Tokenize and add special tokens\n",
        "  input_ids = tokenizer.encode(\n",
        "    text = target_lang_token + text,\n",
        "    return_tensors = 'pt',\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=seq_len\n",
        "  )\n",
        "\n",
        "  return input_ids[0]\n",
        "\n",
        "\n",
        "def encode_target_str(\n",
        "    text, tokenizer, seq_len,\n",
        "    lang_token_map=LANG_TOKEN_MAPPING):\n",
        "\n",
        "  token_ids = tokenizer.encode(\n",
        "    text = text,\n",
        "    return_tensors = 'pt',\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=seq_len)\n",
        "\n",
        "  return token_ids[0]\n",
        "\n",
        "def format_translation_data(translations, lang_token_map, tokenizer, seq_len=128):\n",
        "  langs = list(lang_token_map.keys())\n",
        "\n",
        "  input_lang, target_lang = np.random.choice(langs, size=2, replace=False)\n",
        "\n",
        "  # Get the translations from the batch\n",
        "  input_text = translations[input_lang]\n",
        "  target_text = translations[target_lang]\n",
        "\n",
        "  if input_text is None or target_text is None:\n",
        "    return None\n",
        "\n",
        "  input_token_ids = encode_input_str(\n",
        "      input_text, target_lang, tokenizer, seq_len, lang_token_map\n",
        "  )\n",
        "\n",
        "  target_token_ids = encode_target_str(\n",
        "      target_text, tokenizer, seq_len, lang_token_map\n",
        "  )\n",
        "\n",
        "  return input_token_ids, target_token_ids\n",
        "\n",
        "def transform_batch(batch, lang_token_map, tokenizer):\n",
        "  inputs = []\n",
        "  targets = []\n",
        "\n",
        "  for translation_set in batch['translation']:\n",
        "    formatted_data = format_translation_data(translation_set, lang_token_map, tokenizer, max_seq_len)\n",
        "\n",
        "    if formatted_data is None:\n",
        "      continue\n",
        "\n",
        "    input_ids, target_ids = formatted_data\n",
        "    inputs.append(input_ids.unsqueeze(0))\n",
        "    targets.append(target_ids.unsqueeze(0))\n",
        "\n",
        "  batch_input_ids = torch.cat(inputs).cuda()\n",
        "  batch_target_ids = torch.cat(targets).cuda()\n",
        "\n",
        "  return batch_input_ids, batch_target_ids\n",
        "\n",
        "def get_data_generator(dataset, lang_token_map, tokenizer, batch_size=32):\n",
        "  dataset = dataset.shuffle()\n",
        "  for i in range(0, len(dataset), batch_size):\n",
        "    raw_batch = dataset[i:i+batch_size]\n",
        "    yield transform_batch(raw_batch, lang_token_map, tokenizer)"
      ],
      "metadata": {
        "id": "jo04YVyC94Dk"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_ids, out_ids = format_translation_data(train_dataset[0]['translation'], LANG_TOKEN_MAPPING, tokenizer)\n",
        "\n",
        "print(' '.join(tokenizer.convert_ids_to_tokens(in_ids)))\n",
        "print(' '.join(tokenizer.convert_ids_to_tokens(out_ids)))\n",
        "\n",
        "data_gen = get_data_generator(train_dataset, LANG_TOKEN_MAPPING, tokenizer, 8)\n",
        "data_batch = next(data_gen)\n",
        "\n",
        "print(\"Input shape: \", data_batch[0].shape)\n",
        "print(\"Output shape: \", data_batch[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "Z4LCW2yhBP-k",
        "outputId": "b100a19e-66fd-4400-9d80-1c7e81775e59"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-82f7ba6e74c0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0min_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_translation_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'translation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLANG_TOKEN_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-397e38682d87>\u001b[0m in \u001b[0;36mformat_translation_data\u001b[0;34m(translations, lang_token_map, tokenizer, seq_len)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;31m# Get the translations from the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mtarget_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_lang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtarget_text\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'ru'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 5\n",
        "batch_size = 16\n",
        "print_freq = 50\n",
        "lr = 5e-4\n",
        "n_batches = int(np.ceil(len(train_dataset) / batch_size))\n",
        "total_steps = n_epochs * n_batches\n",
        "\n",
        "n_warmup_steps = int(total_steps * 0.01)"
      ],
      "metadata": {
        "id": "-kAvbG1_Yjot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer mt5\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, n_warmup_steps, total_steps)\n"
      ],
      "metadata": {
        "id": "S60YZBJaaBDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []"
      ],
      "metadata": {
        "id": "NJ-vgtAJasc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, gdataset, max_iters=8):\n",
        "  test_generator = get_data_generator(gdataset, LANG_TOKEN_MAPPING, tokenizer, batch_size)\n",
        "\n",
        "  eval_losses = []\n",
        "  for i, (input_batch, label_batch) in enumerate(test_generator):\n",
        "    if i >= max_iters:\n",
        "      break\n",
        "\n",
        "    model_out = model.forward(\n",
        "        input_ids = input_batch,\n",
        "        labels = label_batch\n",
        "    )\n",
        "    eval_losses.append(model_out.loss.item())\n",
        "\n",
        "  return np.mean(eval_losses)"
      ],
      "metadata": {
        "id": "Blpk1Z79l_4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = eval_model(model, test_dataset)"
      ],
      "metadata": {
        "id": "QKR2OCadnJ02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss"
      ],
      "metadata": {
        "id": "IDKUFhqqnWQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch_idx in range(n_epochs):\n",
        "  # Randomize data order\n",
        "  data_generator = get_data_generator(train_dataset, LANG_TOKEN_MAPPING, tokenizer, batch_size)\n",
        "\n",
        "  for batch_idx, (input_batch, label_batch) in tqdm_notebook(enumerate(data_generator), total=n_batches):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    model_out = model.forward(\n",
        "        input_ids = input_batch,\n",
        "        labels = label_batch)\n",
        "\n",
        "    loss = model_out.loss\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print training update info\n",
        "    if (batch_idx + 1) % print_freq == 0:\n",
        "      avg_loss = np.mean(losses[-print_freq:])\n",
        "      print('Epoch: {} | Step: {} | Avg. loss: {:.3f} | lr: {}'.format(\n",
        "          epoch_idx+1, batch_idx+1, avg_loss, scheduler.get_last_lr()[0]\n",
        "      ))"
      ],
      "metadata": {
        "id": "r7qaSyiUaxp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Translation form\n",
        "input_text = \"This is a test\" # @param {type:\"string\"}\n",
        "output_language = \"ja\" # @param [\"en\", \"uk\", \"ja\"]\n",
        "\n",
        "input_ids = encode_input_str(\n",
        "    text = input_text,\n",
        "    target_lang = output_language,\n",
        "    tokenizer = tokenizer,\n",
        "    seq_len = model.config.max_length,\n",
        "    lang_token_map = LANG_TOKEN_MAPPING)\n",
        "\n",
        "input_ids = input_ids.unsqueeze(0).cuda()\n",
        "\n",
        "output_tokens = model.generate(input_ids, num_beams=20, length_penalty=0.2)\n",
        "\n",
        "print(input_text + ' -> ' + tokenizer.decode(output_tokens[0], skip_special_tokens=True))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "InSgDqoOpdn_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "56ed791b-f5e3-4482-9642-224528bdc697"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-ba2058c474bb>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutput_language\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ja\"\u001b[0m \u001b[0;31m# @param [\"en\", \"uk\", \"ja\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m input_ids = encode_input_str(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtarget_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_language\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-397e38682d87>\u001b[0m in \u001b[0;36mencode_input_str\u001b[0;34m(text, target_lang, tokenizer, seq_len, lang_token_map)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     lang_token_map=LANG_TOKEN_MAPPING):\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtarget_lang_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang_token_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_lang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# Tokenize and add special tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'ja'"
          ]
        }
      ]
    }
  ]
}